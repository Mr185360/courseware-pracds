{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Big Data\n",
    "### Notes, Introduction to Big Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Traditional Data Problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional problems are solved with business intelligence:\n",
    "\n",
    "* skills\n",
    "    * querying\n",
    "    * reporting\n",
    "    * exploration\n",
    "* relational databases\n",
    "    * SQL\n",
    "* historical questions\n",
    "    * SELECT AVG(profit) FROM results\n",
    "    * facts \n",
    "        * if the data is correct, the answers are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Big Data problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def., a non-traditional data problem...\n",
    "\n",
    "> Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by **traditional data-processing application software**. \n",
    "\n",
    "> Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.\n",
    "\n",
    "> Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy.\n",
    "\n",
    "\n",
    "- wikipedia, Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What new challenges brought about this field?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* real-time analysis\n",
    "* complex data\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What situations require real-time analysis?\n",
    "\n",
    "* responsive interaction with customers\n",
    "* IOT devices working with sensor data\n",
    "* ... \n",
    "\n",
    "## E-commerce Example\n",
    "* imagine a web store (eg., amazon)\n",
    "* the website wishes to monitor customer behaviour (eg., adding items to cart, leaving the website without purchasing, etc.)\n",
    "    * monitor these REAL TIME\n",
    "* respond to the \"customer behaviour signals\"\n",
    "    * with some intelligent/adaptive response\n",
    "    * offering discounts, alterting user, drawing user attention, ... \n",
    "    \n",
    "    \n",
    "### The Challenges\n",
    "* each customer generates a significant amount of data\n",
    "    * eg., mouse behaviour, click events, leaving & joining... \n",
    "* thousands of customers using the web store *at the same time*\n",
    "* analysis has to be fast, and responsive *in real time*\n",
    "    * vs., a \"traditional problem\" which can be left to batch / over-night processing\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What types of data are more complex than \"traditional\" rows?\n",
    "* increaed data complexity\n",
    "* images\n",
    "* text\n",
    "    * speech\n",
    "* audio\n",
    "\n",
    "\n",
    "## Satellite Image Analysis for Government & Charity Resource Tracking\n",
    "* Suppose we distribute resources via trucks/cars/etc. in a developing nation\n",
    "    * can we track the efficient/safe/reliable distribution of these?\n",
    "    * also, eg., george cloony: satellite image analysis for early-warning of conflicts\n",
    "    \n",
    "### Challenges\n",
    "* a single satellite image is GBs of data\n",
    "* many images are needed to reconstruct a country/road-system\n",
    "    * > 10 TB of data *per view*\n",
    "* processing requires looking at a \"view of the country\" each day/week/etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are The Three Vs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we decide when a problem is non-traditional? Are there any emprical metrics?\n",
    "\n",
    "* Velocity\n",
    "    * real-time\n",
    "    * problems \"quickly\" become non-traditional\n",
    "        * traditional solutions *do not scale well*\n",
    "    * response required < 1s\n",
    "    * ratio = traditional-query-time / response-time\n",
    "    * is this close-to or more-than 1?\n",
    "        * eg., 60 seconds / 120 secods = 0.5\n",
    "        * eg., 60 seconds / 30 seconds = 2\n",
    "        \n",
    "* Vairety\n",
    "    * data complexity \n",
    "    * problems \"quickly\" become non-traditional\n",
    "        * traditional solutions *do not scale well*\n",
    "    * independent dimensions in the dataset\n",
    "        * independent attributes of the data\n",
    "    * repeated information:\n",
    "        * (lat, long), (address), (geotag)\n",
    "    * large amounts of *relevant* non-repeated information\n",
    "    * image: 1MP is 1,000,000\n",
    "        * even a small image is 1-million dimensions\n",
    "    * if you're considering >20 *relevant*, *indepedent* attributes, may have big-data problem\n",
    "    \n",
    "    \n",
    "* Volume\n",
    "    * traditional solutions *scale well*\n",
    "    * RAM\n",
    "        * how much memory does a query need?\n",
    "        * ratio = query-need / machine-amount\n",
    "            * of the best plausible single machine\n",
    "        * eg., 100 GB / 512 GB\n",
    "            * not a big data problem\n",
    "        * eg., 1 TB / 512 GB\n",
    "            * could be a big data problem\n",
    "            * may also be possible to optimize the query \n",
    "    * Storage\n",
    "        * how big is the entire dataset?\n",
    "        * ratio = disk-size / machine-amount\n",
    "        * eg., 10 TB / 16TB\n",
    "            * not a big data problem\n",
    "            \n",
    "            \n",
    "## Where is the boundary between big-data and traditional?\n",
    "\n",
    "Not fixed. In 2010, 100 GB query may be big data, now *certaintly isn't*. \n",
    "\n",
    "\n",
    "## What other `V`s are also mentioned?\n",
    "* Veracity\n",
    "* Value\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data for an Analyst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Big Data Analysis' connotes prediction, explanation, non-traditional data models: graphs, documents, ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data for an Engineer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing, Peformance, tools, tradeoffs local single-machine, data centres, CAP theorem, configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are these non-traditional data models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional data model is *the table*: row, columns,..\n",
    "\n",
    "Non-Traditional:\n",
    "\n",
    "* key-value pairs\n",
    "    * loose structure\n",
    "* documents\n",
    "    * heirachy\n",
    "* graphs\n",
    "    * order\n",
    "* column stores\n",
    "    * 1000s col\n",
    "* schemaless\n",
    "    * unstructuerd data\n",
    "* exotic:\n",
    "    * time series dbs\n",
    "    * matrices, linear algebra, \n",
    "\n",
    "Often with big data problems, datasets will be stored in a non-tabular form -- even if, by the end, we have a table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, \"not sql\",  now, \"not only sql\".\n",
    "\n",
    "Data systems, and data model (ie., structures), which are non-tabular and are not \"natively\" SQL-based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I understand data models & queries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data model describes how the data is structured. The query brings together datasets, reshapes and summaries; ie., the query structures and processes. \n",
    "\n",
    "\n",
    "There's a tradeoff: you can either structure data *specific* to a need, and make the query simpler & faster; or make the structure general, querying take longer. \n",
    "\n",
    "Relational databases propose a universal data structure (ie., a table); but this *can* have negative performance implications if you're problem requires: \n",
    "* order, \n",
    "* sparcity, \n",
    "* heirachy, \n",
    "* extensive re-structuring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a table?\n",
    "* { (id, age, temp), ... }\n",
    "* { (1, 20, 19), (2, 20, 30), ...}\n",
    "* table = set of rows\n",
    "* row = tuple of values\n",
    "\n",
    "## What are Key-Value pairs?\n",
    "* a single tagged value\n",
    "    * {\"id\": 1}\n",
    "* datasets are bundles a single piece of data with a label\n",
    "    * bag of \n",
    "        * me:   {\"id\": 1}, {\"age\": 20}, {\"temp\": 19}\n",
    "        * you:  {\"id\": 2}, {\"age\": 20}, {\"temp\": 30}\n",
    "\n",
    "## What are Documents?\n",
    "* documents give you heirachy\n",
    "* using key-values in a more structured way\n",
    "* collection of key-value pairs\n",
    "    * where each *value* can itself be such a collection\n",
    "* this enables modelling *heirachical* data\n",
    "    * vs., tabular -- *querying* creates heiarchy\n",
    "    * vs., tabular -- here, we store in a heiarchy\n",
    "    \n",
    "```\n",
    "credit_file = {\n",
    "    \"name\" : \"Michael\",\n",
    "    \"address\": [(\"London\", \"UK\"), ...],\n",
    "    \"loan\" : {\n",
    "        \"amount\": 1000,\n",
    "        \"date\": 1/1/1900\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "## What are Graphs?\n",
    "* graphs give you ordering between datasets (\"rows\")\n",
    "* data is stored in an order\n",
    "    * eg., consider below, storing\n",
    "    * eg., $\\{(Alice, (Michael, Bob)),  (Michael, ()), (Bob, (Alice)), (Eve, (Bob))\\}$\n",
    "        * ie., $\\{(Node, Neighbors)\\}$\n",
    "* tables do not order data\n",
    "    * querying has to impose order\n",
    "    * therefore querying is more efficient if data is stored in an order\n",
    "    \n",
    "```\n",
    "Michael -LIKES-> Alice <-LIKES-> Bob -LIKES-> Eve\n",
    "```\n",
    "\n",
    "* who like Alice?\n",
    "    * {Bob, Eve}\n",
    "\n",
    "## What's a Column-Store?\n",
    "* a table stored in column-form\n",
    "\n",
    "* compare with: { (id, age, temp), ... }\n",
    "* { (1,2), (20, 20), (19, 30), ...}\n",
    "\n",
    "* table = set of columns\n",
    "* querying selects a *subset of columns*\n",
    "    * so choosing, say, 20 / 2000 columns is *efficient* \n",
    "* eg., consider an image: that could be a 1million-col row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a schemaless db / data store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider images: it is not efficient to store, as a row, a 1mil-col image. \n",
    "\n",
    "Rather, leave it on the disk as a binary image file *and while querying* convert to a tabular (ie., a matrix) form. \n",
    "\n",
    "\n",
    "This suggest a need for a storage system which imposes no consistent schema across datasets (ie., files); and leaves the query to do *all* the processing, including even imposing some basic structure. \n",
    "\n",
    "This is just a file system!\n",
    "\n",
    "A schemaless data store is then *just a file system*... a big data version will provide a file system across 100s+ machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the important examples of Big Data tools & techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* schemaless (\"object store\")\n",
    "    * hadoop (on-premise)\n",
    "    * S3 amazon \n",
    "    * Microsoft Object-Store\n",
    "    * Google Cloud Storage\n",
    "* graph\n",
    "    * neo4j\n",
    "* key-value\n",
    "    * redis\n",
    "* document\n",
    "    * mongo\n",
    "    * ...\n",
    "* columnar\n",
    "    * cassandra\n",
    "    * hbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
